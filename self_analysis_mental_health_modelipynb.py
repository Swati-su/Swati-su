# -*- coding: utf-8 -*-
"""Self-Analysis Mental Health Modelipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BIO3jPnsDVI1JXk7JUwjyWICRZ-O-Xjm

<a href="https://colab.research.google.com/github/cdodiya/Mental-Health-Prediction-using-Machine-Learning-Algorithms/blob/main/MentalHealthPredictionUsingMachineLearningAlgorithms.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

Data Loading
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from scipy.stats import randint

# prep
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.datasets import make_classification
from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler

# models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier

# Validation libraries
from sklearn import metrics
from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve
from sklearn.model_selection import cross_val_score

from sklearn.naive_bayes import GaussianNB
from mlxtend.classifier import StackingClassifier

#Neural Network
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RandomizedSearchCV

#Bagging
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier

#Naive bayes
from sklearn.naive_bayes import GaussianNB

#Stacking
from mlxtend.classifier import StackingClassifier

from google.colab import files
uploaded = files.upload()

train_df = pd.read_csv('survey.csv')
print(train_df.shape)
print(train_df.describe())
print(train_df.info())

"""#Data Cleaning"""

#missing data
total = train_df.isnull().sum().sort_values(ascending=False)
percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
print(missing_data)

#dealing with missing data
train_df.drop(['comments'], axis= 1, inplace=True)
train_df.drop(['state'], axis= 1, inplace=True)
train_df.drop(['Timestamp'], axis= 1, inplace=True)

train_df.isnull().sum().max() #just checking that there's no missing data missing...
train_df.head(5)

"""Cleaning NaN"""

# Assign default values for each data type
defaultInt = 0
defaultString = 'NaN'
defaultFloat = 0.0

# Create lists by data tpe
intFeatures = ['Age']
stringFeatures = ['Gender', 'Country', 'self_employed', 'family_history', 'treatment', 'work_interfere',
                 'no_employees', 'remote_work', 'tech_company', 'anonymity', 'leave', 'mental_health_consequence',
                 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview',
                 'mental_vs_physical', 'obs_consequence', 'benefits', 'care_options', 'wellness_program',
                 'seek_help']
floatFeatures = []

# Clean the NaN's
for feature in train_df:
    if feature in intFeatures:
        train_df[feature] = train_df[feature].fillna(defaultInt)
    elif feature in stringFeatures:
        train_df[feature] = train_df[feature].fillna(defaultString)
    elif feature in floatFeatures:
        train_df[feature] = train_df[feature].fillna(defaultFloat)
    else:
        print('Error: Feature %s not recognized.' % feature)
train_df.head()

#Clean 'Gender'
gender = train_df['Gender'].unique()
print(gender)

#Made gender groups
male_str = ["male", "m", "male-ish", "maile", "mal", "male (cis)", "make", "male ", "man","msle", "mail", "malr","cis man", "Cis Male", "cis male"]
trans_str = ["trans-female", "something kinda male?", "queer/she/they", "non-binary","nah", "all", "enby", "fluid", "genderqueer", "androgyne", "agender", "male leaning androgynous", "guy (-ish) ^_^", "trans woman", "neuter", "female (trans)", "queer", "ostensibly male, unsure what that really means"]
female_str = ["cis female", "f", "female", "woman",  "femake", "female ","cis-female/femme", "female (cis)", "femail"]

for (row, col) in train_df.iterrows():

    if str.lower(col.Gender) in male_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='male', inplace=True)

    if str.lower(col.Gender) in female_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='female', inplace=True)

    if str.lower(col.Gender) in trans_str:
        train_df['Gender'].replace(to_replace=col.Gender, value='trans', inplace=True)

#Get rid of bullshit
stk_list = ['A little about you', 'p']
train_df = train_df[~train_df['Gender'].isin(stk_list)]

print(train_df['Gender'].unique())

#complete missing age with mean
train_df['Age'].fillna(train_df['Age'].median(), inplace = True)

# Fill with media() values < 18 and > 120
s = pd.Series(train_df['Age'])
s[s<18] = train_df['Age'].median()
train_df['Age'] = s
s = pd.Series(train_df['Age'])
s[s>120] = train_df['Age'].median()
train_df['Age'] = s

#Ranges of Age
train_df['age_range'] = pd.cut(train_df['Age'], [0,20,30,65,100], labels=["0-20", "21-30", "31-65", "66-100"], include_lowest=True)

#There are only 0.014% of self employed so let's change NaN to NOT self_employed
#Replace "NaN" string from defaultString
train_df['self_employed'] = train_df['self_employed'].replace([defaultString], 'No')
print(train_df['self_employed'].unique())

#There are only 0.20% of self work_interfere so let's change NaN to "Don't know
#Replace "NaN" string from defaultString

train_df['work_interfere'] = train_df['work_interfere'].replace([defaultString], 'Don\'t know' )
print(train_df['work_interfere'].unique())

"""#Encoding Data"""

#Encoding data
labelDict = {}
for feature in train_df:
    le = preprocessing.LabelEncoder()
    le.fit(train_df[feature])
    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    train_df[feature] = le.transform(train_df[feature])
    # Get labels
    labelKey = 'label_' + feature
    labelValue = [*le_name_mapping]
    labelDict[labelKey] =labelValue

for key, value in labelDict.items():
    print(key, value)

#Get rid of 'Country'
train_df = train_df.drop(['Country'], axis= 1)
train_df.head()

"""Testing there aren't any missing data"""

#missing data
total = train_df.isnull().sum().sort_values(ascending=False)
percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)
print(missing_data)

"""Features Scaling: We're going to scale age, because it is extremely different from the other ones.

#Covariance Matrix. Variability comparison between categories of variables
"""

#correlation matrix
corrmat = train_df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);
plt.show()

#treatment correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'treatment')['treatment'].index
cm = np.corrcoef(train_df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

"""#Some charts to see data relationship

**Distribution** and density by Age
"""

# Distribution and density by Age
plt.figure(figsize=(12,8))
sns.distplot(train_df["Age"], bins=24)
plt.title("Distribution and density by Age")
plt.xlabel("Age")

"""Separate by treatment"""

import seaborn as sns
import matplotlib.pyplot as plt

# Corrected: Use `height` instead of `size`
g = sns.FacetGrid(train_df, col='treatment', height=5)

# Corrected: Replace `sns.distplot()` with `sns.histplot()` or `sns.kdeplot()`
g.map(sns.histplot, "Age", bins=20, kde=True)  # Use kde=False if you only want a histogram

plt.show()

"""How many people has been treated?"""

plt.figure(figsize=(12,8))
labels = labelDict['label_Gender']
g = sns.countplot(x="treatment", data=train_df)
g.set_xticklabels(labels)

plt.title('Total Distribution by treated or not')

"""Nested barplot to show probabilities for class and sex"""

import seaborn as sns
import matplotlib.pyplot as plt

o = labelDict['label_age_range']

# Replace `factorplot` with `catplot`
g = sns.catplot(
    x="age_range",
    y="treatment",
    hue="Gender",
    data=train_df,
    kind="bar",
    ci=None,
    height=5,  # Replaces `size`
    aspect=2,
    legend=True  # `legend_out` is not needed
)

g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Age')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

plt.show()

"""Barplot to show probabilities for family history"""

import seaborn as sns
import matplotlib.pyplot as plt

o = labelDict['label_family_history']

# Replace `factorplot` with `catplot`
g = sns.catplot(
    x="family_history",
    y="treatment",
    hue="Gender",
    data=train_df,
    kind="bar",
    ci=None,
    height=5,  # Use `height` instead of `size`
    aspect=2
)

g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Family History')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

plt.show()

"""Barplot to show probabilities for care options"""

import seaborn as sns
import matplotlib.pyplot as plt

o = labelDict['label_care_options']

# Replace factorplot with catplot
g = sns.catplot(
    x="care_options",
    y="treatment",
    hue="Gender",
    data=train_df,
    kind="bar",
    ci=None,
    height=5,  # Replace `size` with `height`
    aspect=2
)

g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Care options')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

plt.show()

"""Barplot to show probabilities for benefits


"""

import seaborn as sns
import matplotlib.pyplot as plt

o = labelDict['label_benefits']

# Use catplot instead of factorplot
g = sns.catplot(
    x="care_options",
    y="treatment",
    hue="Gender",
    data=train_df,
    kind="bar",
    ci=None,
    height=5,  # Replacing `size` with `height`
    aspect=2
)

g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Benefits')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

plt.show()

"""Barplot to show probabilities for work interfere


"""

import seaborn as sns
import matplotlib.pyplot as plt

o = labelDict['label_work_interfere']

# Use catplot instead of factorplot
g = sns.catplot(
    x="work_interfere",
    y="treatment",
    hue="Gender",
    data=train_df,
    kind="bar",
    ci=None,
    height=5,  # Replacing `size` with `height`
    aspect=2
)

g.set_xticklabels(o)

plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Work interfere')

# Replace legend labels
new_labels = labelDict['label_Gender']
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

# Positioning the legend
g.fig.subplots_adjust(top=0.9, right=0.8)

plt.show()

"""#Scaling and Fitting

Features Scaling We're going to scale age, because is extremely different from the othere ones.
"""

# Scaling Age
scaler = MinMaxScaler()
train_df['Age'] = scaler.fit_transform(train_df[['Age']])
train_df.head()

"""Spilitting Dataset"""

# define X and y
feature_cols = ['Age', 'Gender', 'family_history', 'benefits', 'care_options', 'anonymity', 'leave', 'work_interfere']
X = train_df[feature_cols]
y = train_df.treatment

# split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

# Create dictionaries for final graph
# Use: methodDict['Stacking'] = accuracy_score
methodDict = {}
rmseDict = ()

# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

labels = []
for f in range(X.shape[1]):
    labels.append(feature_cols[f])

# Plot the feature importances of the forest
plt.figure(figsize=(12,8))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), labels, rotation='vertical')
plt.xlim([-1, X.shape[1]])
plt.show()

"""Logistic Regression"""

# Import necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Assuming X and y are your feature matrix and target labels, respectively.
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert categorical labels to numeric if needed
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

# Dictionary to store model performance
methodDict = {}

def logisticRegression():
    # Initialize Logistic Regression model
    logreg = LogisticRegression(max_iter=500, solver='liblinear')  # Increased max_iter for convergence
    logreg.fit(X_train, y_train)  # Train the model

    # Make predictions
    y_pred_class = logreg.predict(X_test)
    y_pred_prob = logreg.predict_proba(X_test)[:, 1]  # Probability predictions for ROC-AUC

    # Calculate Evaluation Metrics
    accuracy = accuracy_score(y_test, y_pred_class)
    precision = precision_score(y_test, y_pred_class, average='weighted')
    recall = recall_score(y_test, y_pred_class, average='weighted')
    f1 = f1_score(y_test, y_pred_class, average='weighted')

    # Compute ROC-AUC Score (Handles both binary and multi-class cases)
    if len(set(y_test)) > 2:  # Multi-class classification
        roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test), multi_class='ovr')
    else:  # Binary classification
        roc_auc = roc_auc_score(y_test, y_pred_prob)

    # Print Evaluation Metrics
    print("\nðŸ”¹ Logistic Regression Model Performance ðŸ”¹")
    print(f"âœ… Accuracy: {accuracy:.4f}")
    print(f"âœ… Precision: {precision:.4f}")
    print(f"âœ… Recall: {recall:.4f}")
    print(f"âœ… F1-score: {f1:.4f}")
    print(f"âœ… ROC-AUC Score: {roc_auc:.4f}\n")

    # Store Accuracy for Graph Plotting
    methodDict['Log. Regression'] = accuracy * 100

# Run the function
logisticRegression()

# Print the stored accuracy
print("ðŸ“Š Stored Accuracy in methodDict:", methodDict)

from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression()

# Fit the model (assuming X_train and y_train are defined)
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)

"""KNeighbors Classifier"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define tuning function
def tuningRandomizedSearchCV(model, param_dist, cv=5, n_iter=10):
    """Perform Randomized Search Cross Validation to find the best parameters."""
    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_iter, cv=cv, scoring='accuracy', random_state=42, n_jobs=-1)
    random_search.fit(X_train, y_train)
    print("Best parameters found: ", random_search.best_params_)
    return random_search.best_params_

# Initialize methodDict for storing results
methodDict = {}

# Define the K-Nearest Neighbors function
def Knn():
    # Create initial KNN model
    knn = KNeighborsClassifier(n_neighbors=5)

    # Define the parameter values that should be searched
    k_range = list(range(1, 31))
    weight_options = ['uniform', 'distance']

    # Define parameter distribution
    param_dist = dict(n_neighbors=k_range, weights=weight_options)

    # Perform Randomized Search CV
    best_params = tuningRandomizedSearchCV(knn, param_dist)

    # Train a KNeighborsClassifier model with the best-found parameters
    knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'])
    knn.fit(X_train, y_train)

    # Make class predictions for the testing set
    y_pred_class = knn.predict(X_test)

    # Evaluation metrics
    accuracy = accuracy_score(y_test, y_pred_class)
    precision = precision_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    recall = recall_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    f1 = f1_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class

    # ROC-AUC Score (Only applicable if probability predictions are available)
    try:
        roc_auc = roc_auc_score(y_test, knn.predict_proba(X_test), multi_class='ovr')  # 'ovr' for one-vs-rest
    except:
        roc_auc = None  # ROC-AUC is not defined for some cases

    # Printing all the metrics
    print("\nðŸ”¹ K-Neighbors Model Performance ðŸ”¹")
    print(f"âœ… Accuracy: {accuracy:.4f}")
    print(f"âœ… Precision: {precision:.4f}")
    print(f"âœ… Recall: {recall:.4f}")
    print(f"âœ… F1-score: {f1:.4f}")
    if roc_auc is not None:
        print(f"âœ… ROC-AUC Score: {roc_auc:.4f}")
    else:
        print("âœ… ROC-AUC Score: Not Applicable")

    # Save the accuracy to methodDict for final graph
    methodDict['K-Neighbors'] = accuracy * 100

# Run the function
Knn()

# Print the stored accuracy
print("ðŸ“Š Stored Accuracy in methodDict:", methodDict)

from sklearn.neighbors import KNeighborsClassifier

# Initialize the model
knn = KNeighborsClassifier(n_neighbors=5)  # You can change the number of neighbors

# Train the model (assuming X_train and y_train are defined)
knn.fit(X_train, y_train)

# Make predictions
predictions = knn.predict(X_test)

"""Decision Tree classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from scipy.stats import randint

def treeClassifier():
    # Calculating the best parameters
    tree = DecisionTreeClassifier()
    featuresSize = len(feature_cols)
    param_dist = {
        "max_depth": [3, None],
        "max_features": randint(1, featuresSize),
        "min_samples_split": randint(2, 9),
        "min_samples_leaf": randint(1, 9),
        "criterion": ["gini", "entropy"]
    }
    tuningRandomizedSearchCV(tree, param_dist)

    # Train a decision tree model on the training set
    tree = DecisionTreeClassifier(max_depth=3, min_samples_split=8, max_features=6, criterion='entropy', min_samples_leaf=7)
    tree.fit(X_train, y_train)

    # Make class predictions for the testing set
    y_pred_class = tree.predict(X_test)

    # Calculate Evaluation Metrics
    accuracy = accuracy_score(y_test, y_pred_class)
    precision = precision_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    recall = recall_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    f1 = f1_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class

    # For multi-class classification, ROC-AUC needs a slight adjustment
    try:
        roc_auc = roc_auc_score(y_test, tree.predict_proba(X_test), multi_class='ovr')  # 'ovr' for one-vs-rest
    except:
        roc_auc = None  # ROC-AUC is not defined for some cases, e.g., for binary classification without probabilities

    # Printing all the metrics
    print("\nðŸ”¹ Decision Tree Classifier Model Performance ðŸ”¹")
    print(f"âœ… Accuracy: {accuracy:.4f}")
    print(f"âœ… Precision: {precision:.4f}")
    print(f"âœ… Recall: {recall:.4f}")
    print(f"âœ… F1-score: {f1:.4f}")
    if roc_auc is not None:
        print(f"âœ… ROC-AUC Score: {roc_auc:.4f}")
    else:
        print("âœ… ROC-AUC Score: Not Applicable")

    # Save the accuracy to methodDict for final graph
    methodDict['Decision Tree Classifier'] = accuracy * 100

# Run the function
treeClassifier()

# Print the stored accuracy
print("ðŸ“Š Stored Accuracy in methodDict:", methodDict)

from sklearn.tree import DecisionTreeClassifier

# Initialize the model
tree_clf = DecisionTreeClassifier()

# Train the model (assuming X_train and y_train are defined)
tree_clf.fit(X_train, y_train)

# Make predictions
predictions = tree_clf.predict(X_test)

"""Random Forests"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from scipy.stats import randint

def randomForest():
    # Calculating the best parameters
    forest = RandomForestClassifier(n_estimators=20)

    featuresSize = len(feature_cols)
    param_dist = {
        "max_depth": [3, None],
        "max_features": randint(1, featuresSize),
        "min_samples_split": randint(2, 9),
        "min_samples_leaf": randint(1, 9),
        "criterion": ["gini", "entropy"]
    }
    tuningRandomizedSearchCV(forest, param_dist)

    # Building and fitting the RandomForest model
    forest = RandomForestClassifier(max_depth=None, min_samples_leaf=8, min_samples_split=2, n_estimators=20, random_state=1)
    my_forest = forest.fit(X_train, y_train)

    # Make class predictions for the testing set
    y_pred_class = my_forest.predict(X_test)

    # Calculate Evaluation Metrics
    accuracy = accuracy_score(y_test, y_pred_class)
    precision = precision_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    recall = recall_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class
    f1 = f1_score(y_test, y_pred_class, average='weighted')  # 'weighted' for multi-class

    # For multi-class classification, ROC-AUC needs a slight adjustment
    try:
        roc_auc = roc_auc_score(y_test, my_forest.predict_proba(X_test), multi_class='ovr')  # 'ovr' for one-vs-rest
    except:
        roc_auc = None  # ROC-AUC is not defined for some cases, e.g., for binary classification without probabilities

    # Printing all the metrics
    print("\nðŸ”¹ Random Forest Model Performance ðŸ”¹")
    print(f"âœ… Accuracy: {accuracy:.4f}")
    print(f"âœ… Precision: {precision:.4f}")
    print(f"âœ… Recall: {recall:.4f}")
    print(f"âœ… F1-score: {f1:.4f}")
    if roc_auc is not None:
        print(f"âœ… ROC-AUC Score: {roc_auc:.4f}")
    else:
        print("âœ… ROC-AUC Score: Not Applicable")

    # Save the accuracy to methodDict for final graph
    methodDict['Random Forest'] = accuracy * 100

# Run the function
randomForest()

# Print the stored accuracy
print("ðŸ“Š Stored Accuracy in methodDict:", methodDict)

from sklearn.ensemble import RandomForestClassifier

# Initialize the model with fewer estimators
rf_clf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)

# Train the model
rf_clf.fit(X_train, y_train)

# Make predictions
predictions = rf_clf.predict(X_test)

"""Bagging"""

# Import necessary libraries
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Check if bagging is previously defined and clear it
try:
    del bagging  # Remove any previous conflicting definitions
except NameError:
    pass  # If bagging is not defined yet, no problem

# Define the bagging function
def bagging(X_train, y_train, X_test, y_test, methodDict):
    # Initialize and train BaggingClassifier
    bag = BaggingClassifier(DecisionTreeClassifier(), max_samples=1.0, max_features=1.0, bootstrap_features=False)
    bag.fit(X_train, y_train)

    # Make class predictions for the test set
    y_pred_class = bag.predict(X_test)

    # Evaluate model performance
    accuracy = accuracy_score(y_test, y_pred_class)  # Ensure accuracy function is used correctly

    # Store results in methodDict
    methodDict['Bagging'] = accuracy * 100

    return methodDict  # Return updated dictionary

# Example usage:
methodDict = {}  # Initialize dictionary

# Ensure X_train, y_train, X_test, y_test are defined before calling the function
methodDict = bagging(X_train, y_train, X_test, y_test, methodDict)

# Print the result to check
print(methodDict)

"""Boosting"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

def boosting():
    # Building and fitting
    clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)
    boost = AdaBoostClassifier(estimator=clf, n_estimators=500)
    boost.fit(X_train, y_train)

boosting()

boosting()

"""#Predicting with Neural Network

Create input function
"""

import tensorflow as tf
import argparse

print(tf.__version__)  # Check TensorFlow version

print(tf.__version__)

batch_size = 100
train_steps = 1000

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

def train_input_fn(features, labels, batch_size):
    """An input function for training"""
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle, repeat, and batch the examples.
    return dataset.shuffle(1000).repeat().batch(batch_size)

def eval_input_fn(features, labels, batch_size):
    """An input function for evaluation or prediction"""
    features=dict(features)
    if labels is None:
        # No labels, use only features.
        inputs = features
    else:
        inputs = (features, labels)

    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices(inputs)

    # Batch the examples
    assert batch_size is not None, "batch_size must not be None"
    dataset = dataset.batch(batch_size)

    # Return the dataset.
    return dataset

"""Define the feature columns"""

# Define Tensorflow feature columns
age = tf.feature_column.numeric_column("Age")
gender = tf.feature_column.numeric_column("Gender")
family_history = tf.feature_column.numeric_column("family_history")
benefits = tf.feature_column.numeric_column("benefits")
care_options = tf.feature_column.numeric_column("care_options")
anonymity = tf.feature_column.numeric_column("anonymity")
leave = tf.feature_column.numeric_column("leave")
work_interfere = tf.feature_column.numeric_column("work_interfere")
feature_columns = [age, gender, family_history, benefits, care_options, anonymity, leave, work_interfere]

"""Instantiate an Estimator

Train the model
"""

model.fit(X_train, y_train)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(solver='saga', warm_start=True)

from sklearn.linear_model import SGDClassifier
import numpy as np

# Initialize model with log_loss (equivalent to Logistic Regression)
model = SGDClassifier(loss="log_loss", random_state=42)

# Number of training steps and batch size
train_steps = 10  # Adjust as needed
batch_size = 32   # Adjust as needed

# Perform batch-wise training
for i in range(train_steps):
    indices = np.random.choice(len(X_train), batch_size, replace=False)

    # Call partial_fit() with classes only in the first iteration
    if i == 0:
        model.partial_fit(X_train.iloc[indices], y_train.iloc[indices], classes=np.unique(y_train))
    else:
        model.partial_fit(X_train.iloc[indices], y_train.iloc[indices])

# Check model status
print("Training completed successfully!")

"""Evaluate the model"""

# Evaluate the model using score()
accuracy = model.score(X_test, y_test)

print(f'\nTest set accuracy: {accuracy:.2f}\n')

# Store accuracy in methodDict
methodDict['Neural Network'] = accuracy * 100

"""Making predictions (inferring) from the trained model"""

predictions = model.predict(X_train)
predictions = list(predictions)  # Convert to a list if needed

# Dictionary for predictions
col1 = []  # Index
col2 = []  # Prediction
col3 = []  # Expected (Actual)

# Iterate over X_train index, y_train labels, and predictions
for idx, actual, pred in zip(X_train.index, y_train, predictions):
    col1.append(idx)     # Store index
    col2.append(pred)    # Store predicted value
    col3.append(actual)  # Store actual value

# Create a DataFrame
results = pd.DataFrame({'index': col1, 'prediction': col2, 'expected': col3})

# Display the first few rows
results.head()

"""#Success method plot"""

def plotSuccess():
    s = pd.Series(methodDict)
    s = s.sort_values(ascending=False)
    plt.figure(figsize=(12,8))
    #Colors
    ax = s.plot(kind='bar')
    for p in ax.patches:
        ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))
    plt.ylim([70.0, 90.0])
    plt.xlabel('Method')
    plt.ylabel('Percentage')
    plt.title('Success of methods')

    plt.show()

plotSuccess()